{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions, Text Normalization, and Edit Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, a __regular expression__ is algebraic notation for characterizing sets of strings. A _pattern_ searches through a _corpus_ of texts. The unix tool `grep` takes a regex and returns every line of the corpus that matches the expression.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex is __case sensitive__. To use a _disjunction_ of expressions, use `[a b]` to combine multiple conditions. The dash (`-`) can be used to specify a range. The caret `^` is used to express _negation_. The question mark `?` is used to denote zero or one instances, and the __Kleene__ `*` is used to express zero or more. The __Kleene__ `+` is then used to express one or more. The __wildcard__ expression `.` matche _any_ character except a carriage return. `.` and `+` are often used in conjunction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Anchors__ signify particular places in a string. `^` matches the start of a line when outisde of brackets and `$` the end. Additionally, `\\b` matches a word boundary and `\\B` matches a non boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __disjunction__ operator `|` signifies \"either / or.\" Parenthese can be used to signify _precedence_ in operators. The pattern `/gupp(y|ies)` would first match for suffix disjunction and then the larger word. Operators that match _as much_ as possible are __greedy__, while those that match as little as possible are __non-greedy__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main types of erros are __false positives__ and __false negatives__. Reducing overall error rate thus involves two antagonistic efforts: increasing __precision__ by minimizing false positives and increasing __recall__ by reducing false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitution, Capture, and Lookahead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Substitutions__ are commonly implemented in the form of `s/regex/pattern/`. To make this easier, the __number__ operator `\\` is used as back-reference, such as in the form of `s/([0-9]+)/<\\1>`. This use of parentheses to store a pattern in memory is a __capture group__, where the resulting match is stored in a numbered __register__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __lookahead__ assertion makes use of the operator `(?= pattern)` but is _zero-wdith_, which means the match pointer doesn't advance. Negative lookahead is commonly used when parsing a complex pattern and trying to rule out some special case. For example: to match any single word that doesn't start with \"Volcano\": `/^(?!Volcano)[a-zA-z]+/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An __utterance__ is the spoken correlate of a sentence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I do uh main- mainly business data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This utterance has two kinds of __disfluencies__: the broken-off word _main_ is a __fragment__, and words like _uh_ and _uhm_ are __fillers__. Disfluencies should be kept or discarded depending on the purpose. They may be discarded in transcription or kept in recognition, since they might signal the restart of a clause or idea. A __lemma__ is a set of lexical forms with the same stem, major part-of-speech, and word sense. Inflected forms like _cats_ and _cat?_ share _cat_ as their lemma. The __word-form__ is the full inflected, or derived form of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Types__ are the number of distinct words in a corpus; if the set of words in the vocabulary is $V$, the number of types is the vocabulary size $|V|$. __Tokens__ are the total number $N$ of running words. The larger the corpora, the more words types there tend to be. This relationship between the number of types $|V|$ and number of tokes $N$ is __Herdan's Law__, or __Heap's Law__. For positive constants $k, \\beta$ that satisfy $0 < \\beta < 1$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "|V| = kN^{\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of $\\beta$ depends on the corpus size and genre, but generally $\\beta$ ranges from .67 to .75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text normalization is generally split into a 3-part process:\n",
    "1. Tokenizing (segmenting) words\n",
    "2. Normalizing word formats\n",
    "3. Segmenting sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenization__ segments running text into words. Generally, punctuation is kep as separate tokens; commas are useful information for parsers and periods indicate sentence boundaries. __Clitic__ contractions marked by apostrophes often need to be separated, such as as `what're` into `what` `are`. A clitic is a part of a word that can't stand on its own, and can only occur when attached to another. Depending on appication , tokenization algorithms may also need to tokenize multi-word expressions, like `New York City`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common standard is the __Penn Treebank__ tokenization standard, which expands clitics, keeps hyphenated words together, and separates out all punctuation. In practice, tokenization has to be run before any other processing, and has to be very efficient. The standard method is to use deterinistic algortihms based on regex compiled into  finite state automata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of defining tokens as words, bounded by spaces in orthographies with spaces, sometimes it's useful to have variable sized tokens. A _morpheme_ is the smallest unit of a language; `unluckiest` contains the morphemes `un-`, `likely`, and `-est`. One reason to have __subword__ tokens is to deal with unknown words. If a training corpus contains `low` and `lower`, but not `lowest`, the system may be confused. Tokenizing frequent morphemes like `est` would then be allow combining subwords to form new words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Byte-pair encoding__ (__BPE__) iteratively merges frequent pairs of charavters. The algortihm starts with the set of symbol equal to the set of characters. Each word is represented as a sequence of chracters plus a special end-of-word symbol `_`. At each step, the number of symnol pairs is counted and the most frequently occuring pair is merged into a single symbol: `(A, B)` to `(AB)`. This continues until $k$ merges are performed, where $k$ is predefined. The resulting symbol set then consists of the original character set plus $k$ new symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, BPE is run with many thousands of merges on a large input dictionary. The result is that most words end up represented as their own full symbols, and only rare and unknown words end up having to be formed by combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape('_'.join(pairs))\n",
    "    p = re.compile(r'(?>!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordpiece and Greedy Tokenizaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __wordpiece__ algorithm starts with simple tokenization (such as by whitespace) into rough words and then into subword tokens. The wordpiece model differed from BPE in that the boundary token `_` appears at the beginning rather than the end. Rather than merging the pairs that are the most _frequent_ wordpiece instead merges the pairs that give the training corpus the highest _probability_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the BERT wordpiece segmenter, word-initial subwords are distinguished from those that do not start words by marking internal subwords with `##`. For example, `unaffable` woudl be split into `'un', '\\#\\#aff', '\\#\\#able'`. Then each token string is tokenized using __maximum matching__ (__MaxMatch__), a greedy longest-match-first algorithm, which differs from BPE by not going in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxMatch(string :str, dictionary: dict) -> list:\n",
    "    if not str:\n",
    "        return []\n",
    "    for i in range(len(string) - 1):\n",
    "        first, remainder = string[:i], string[i:]\n",
    "        if first in dictionary:\n",
    "            return list(first, MaxMatch(remainder, dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization, Lemmatization, and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word __normalization__ puts words / tokens in a standard format, choosing a single normal form for words with multiple forms like `USA` and `US`, useful for information retrieval. __Case folding__ is another type of normalization maps everything to a standard case; `Woodchuck` to `woodchuck`, which is helpful for retrieval or speech recognition. For sentiment analysis and other classification tasks, case can be helpful and case folding is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lemmatization__ is the task of determining that two words have the same root. Words `am`, `are`, and `is` share lemma `be`. The lemmatized form of `he is reading detective stories` would be `he be read detective story`. Sophisticated methods for lemmatization involve complete __morphological parsing__ of the word, wherein words are decomposed into _morphemes_. There are two broad classes of morphemes: __stems__, the central morpheme of the word that supplies the main meaning, and __affixes__, that add additional meanings of various kinds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Porter Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since lemmatization can be complex, a naive version called __stemming__ essentially just removes word-final affixes. The widely used Porter stemmer uses a series of rewrite rules run in series as a _cascade_, in which the ouput of each pass is fed as input to the next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sentence segmentation__ uses cues for segmenting a larger corpus into sentences. Question marks and exclamation points are generally unambiguous markers of bounds, but periods are more ambiguous. In general, sentence tokenization methods work by first deciding whether a period is part of a word or a sentence boundary. An abbreviation dictionary can help determine whether the period is part of a common abbreviation; hand-built or machine-learned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Edit Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Edit distance__ gives a way to quantify intuitions about string similarity. The _minimum_ edit distance between two strings is the minimum number of edit operations (insertion, deletion, substituion) needed to transform one string into another. The gap between `intention` and `execution`is 5. Cost or weights can be assigned to each operation. The __Levenshtein__ distance between sequences is the simplest, assigning 1 to each operation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the minimum edit distance can be framed as a search task that uses _dynamic_ programming, applying a table-driven method to solve problems by combining solutions to sub-problems. In trying to find the minimum edit distance between `intention` and `execution`, there may exist an intermediate string `exention` on the optimal path. Dynamic programming asserts that the full optimal sequence must also include the optimal path from `intention` to `exention`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, the minimum edit distance between source string $X$ of length $n$ and target string $Y$ of length $m$ is defined $D[n, m]$, where $D[i, j]$ is the edit distance between $X[1\\ldots i]$ and $Y[1\\ldots j]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D[n, m]$ can then be computed bottom up by combining solutions to sub-problems. In the base case with a source substring with length $i$ and an empty target string, going from $i$ character to 0 requires $i$ deletes. Conversely, a target string with length $j$ but an empty source requires $j$ inserts. Given $D[i, j]$ for small values of $i, j$, larger values can then be computed by taking the minimum of the three possible paths through the matrix which arrive there:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D[i, j] = \n",
    "\\text{min}\n",
    "\\begin{cases}\n",
    "    D[i - 1, j] + \\text{cost}_{\\text{delete}}(\\text{source}[i]) \\\\\n",
    "    D[i, j - 1] + \\text{cost}_{\\text{insert}}(\\text{target}[j]) \\\\\n",
    "    D[i - 1, j - 1] + \\text{cost}_{\\text{substitute}}(\\text{source}[i], \\, \\text{target}[j])\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edit distance algorithm can also provide the minimum cost __alignment__ between two strings, by visualizing an alignment as a path through the edit distance matrix. The minimum edit distance algorithm is modified to store backpointers in each cell. In a __backtrace__, pointers are followed starting from the last cell through the matrix. Each complete path is a minimum distance alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Regular expressions__ are fundamental tools for language processing, enabling __text normalization__ tasks like _word segmentation_, _normalization_, _sentence segmentation_, and _stemming_. The __minimum edit distance__ is important metric for comparing strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic operations in regex include __concatenation__ of symbols, __disjunction__ (`[]`, `|`, `.`), __counter__ (`*`, `+`, `{n, m}`), __anchor__ (`^`, `$`), and __precedence__ (`()`) operators\n",
    "- __Word tokenization__ and __normalization__ are generally done by cascades of simple regex substituions or finite automata\n",
    "- The _Porter algorithm_ is a simple and efficient way of __stemming__, stripping off affixes\n",
    "- The __minimum edit distance__ between strings is the minimum number of operations to edit one into the other, computed by finding an __alignment__ of strings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
